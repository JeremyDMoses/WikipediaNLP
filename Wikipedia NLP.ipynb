{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b85c0b0",
   "metadata": {},
   "source": [
    "# Wikipedia NLP Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook performs comprehensive NLP analysis on a corpus of Wikipedia articles, including:\n",
    "- Text preprocessing and cleaning\n",
    "- Document clustering (KMeans, DBSCAN, OPTICS)\n",
    "- Word embeddings (GloVe)\n",
    "- Dimensionality reduction and visualization\n",
    "- Multi-label classification\n",
    "\n",
    "## Dataset\n",
    "- **Source**: Wikipedia articles\n",
    "- **Size**: 289 documents\n",
    "- **Features**: Article text, categories, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ae051-bc93-42f9-afc7-bd3a0b64a519",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.12' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import wikipediaapi as wiki_api\n",
    "\n",
    "class WikipediaReader():\n",
    "    def __init__(self, dir = \"articles\"):\n",
    "        self.pages = set()\n",
    "        self.article_path = os.path.join(\"./\", dir)\n",
    "        self.wiki = wiki_api.Wikipedia(user_agent = 'jmoses126@gmail.com',\n",
    "                language = 'en',\n",
    "                extract_format=wiki_api.ExtractFormat.WIKI)\n",
    "        try:\n",
    "            os.mkdir(self.article_path)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the reader by clearing all stored pages.\"\"\"\n",
    "        self.pages = set()\n",
    "\n",
    "    def _get_page_title(self, article):\n",
    "        return re.sub(r'\\s+','_', article)\n",
    "\n",
    "    def add_article(self, article):\n",
    "        try:\n",
    "            page = self.wiki.page(self._get_page_title(article))\n",
    "            if page.exists():\n",
    "                self.pages.add(page)\n",
    "                return(page)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def list(self):\n",
    "        return self.pages\n",
    "\n",
    "    def process(self, update=False):\n",
    "        for page in self.pages:\n",
    "            filename = re.sub('\\\\s+', '_', f'{page.title}')\n",
    "            filename = re.sub(r'[\\(\\):]','', filename)\n",
    "            file_path = os.path.join(self.article_path, f'{filename}.txt')\n",
    "            if update or not os.path.exists(file_path):\n",
    "                print(f'Downloading {page.title} ...')\n",
    "                content = page.text\n",
    "                with open(file_path, 'w') as file:\n",
    "                    file.write(content)\n",
    "            else:\n",
    "                print(f'Not updating {page.title} ...')\n",
    "\n",
    "    def crawl_pages(self, article, depth = 3, total_number = 1000):\n",
    "        print(f'Crawl {total_number} :: {article}')\n",
    "\n",
    "        page = self.add_article(article)\n",
    "        childs = set()\n",
    "\n",
    "        if page:\n",
    "            for child in page.links.keys():\n",
    "                if len(self.pages) < total_number:\n",
    "                    print(f'Add article {len(self.pages)}/{total_number} {child}')\n",
    "                    self.add_article(child)\n",
    "                    childs.add(child)\n",
    "\n",
    "        depth -= 1\n",
    "        if depth > 0:\n",
    "            for child in sorted(childs):\n",
    "                if len(self.pages) < total_number:\n",
    "                    self.crawl_pages(child, depth, len(self.pages))\n",
    "\n",
    "    def get_categories(self, title):\n",
    "        page = self.add_article(title)\n",
    "        if page:\n",
    "            if (list(page.categories.keys())) and (len(list(page.categories.keys())) > 0):\n",
    "                categories = [c.replace('Category:','').lower() for c in list(page.categories.keys())\n",
    "                   if c.lower().find('articles') == -1\n",
    "                   and c.lower().find('pages') == -1\n",
    "                   and c.lower().find('wikipedia') == -1\n",
    "                   and c.lower().find('cs1') == -1\n",
    "                   and c.lower().find('webarchive') == -1\n",
    "                   and c.lower().find('dmy dates') == -1\n",
    "                   and c.lower().find('short description') == -1\n",
    "                   and c.lower().find('commons category') == -1\n",
    "\n",
    "                ]\n",
    "                return dict.fromkeys(categories, 1)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c497b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e3d93-d613-4339-a4a6-a831ea1f01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from  nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "from time import time\n",
    "\n",
    "class WikipediaCorpus(CategorizedPlaintextCorpusReader):\n",
    "    \n",
    "    def __init__(self, root, fileids, cat_pattern=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the WikipediaCorpus reader.\n",
    "        \n",
    "        :param root: The root directory for corpus files\n",
    "        :param fileids: File pattern(s) to match\n",
    "        :param cat_pattern: Regex pattern to extract categories from filenames\n",
    "        \"\"\"\n",
    "        super().__init__(root, fileids, cat_pattern=cat_pattern, **kwargs)\n",
    "\n",
    "    def vocab(self):\n",
    "        return nltk.FreqDist(re.sub(r'[^A-Za-z0-9,;\\.]+', ' ', word).lower() for word in corpus.words())\n",
    "\n",
    "    def max_words(self):\n",
    "        max = 0\n",
    "        for doc in self.fileids():\n",
    "            l = len(self.words(doc))\n",
    "            max = l if l > max else max\n",
    "        return max\n",
    "\n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        started = time()\n",
    "\n",
    "        return {\n",
    "            'files': len(self.fileids()),\n",
    "            'paras': len(self.paras()),\n",
    "            'sents': len(self.sents()),\n",
    "            'words': len(self.words()),\n",
    "            'vocab': len(self.vocab()),\n",
    "            'max_words': self.max_words(),\n",
    "            'time': time()-started\n",
    "            }\n",
    "        pass\n",
    "\n",
    "corpus = WikipediaCorpus(root='articles', fileids=r'[^\\\\.ipynb].*', cat_pattern=r'[.*]')\n",
    "print(corpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc07470-33b5-4b20-aaf5-7720356f0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# Fix SSL certificate issue for NLTK downloads on macOS\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "class WikipediaPlaintextCorpus(CategorizedPlaintextCorpusReader):\n",
    "    def __init__(self, root_path):\n",
    "        # Need to provide both root AND fileids\n",
    "        super().__init__(root_path, r'.*\\.txt', cat_pattern=r'.*')\n",
    "    \n",
    "    def vocab(self):\n",
    "        return nltk.FreqDist(re.sub(r'[^A-Za-z0-9,;\\.]+', ' ', word).lower() for word in self.words())\n",
    "    \n",
    "    def max_words(self):\n",
    "        max_count = 0\n",
    "        for doc in self.fileids():\n",
    "            word_count = len(self.words(doc))\n",
    "            max_count = word_count if word_count > max_count else max_count\n",
    "        return max_count\n",
    "    \n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        started = time()\n",
    "        \n",
    "        return {\n",
    "            'files': len(self.fileids()),\n",
    "            'paras': len(self.paras()),\n",
    "            'sents': len(self.sents()),\n",
    "            'words': len(self.words()),\n",
    "            'vocab': len(self.vocab()),\n",
    "            'max_words': self.max_words(),\n",
    "            'time': time() - started\n",
    "        }\n",
    "\n",
    "root_path = './articles'\n",
    "corpus = WikipediaPlaintextCorpus(root_path)\n",
    "print(corpus.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf53a6",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "Load required libraries and define custom classes for the analysis pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2weoayyn8h",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class WikipediaCorpusTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A scikit-learn compatible transformer that loads Wikipedia corpus data.\n",
    "    This class is designed to be used in a sklearn Pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_path):\n",
    "        self.root_path = root_path\n",
    "        self.corpus = None\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        \"\"\"Load the corpus from the root path.\"\"\"\n",
    "        self.corpus = WikipediaPlaintextCorpus(self.root_path)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        \"\"\"Return the list of file IDs from the corpus.\"\"\"\n",
    "        if self.corpus is None:\n",
    "            self.fit()\n",
    "        return self.corpus.fileids()\n",
    "\n",
    "    def get_corpus(self):\n",
    "        \"\"\"Return the loaded corpus object.\"\"\"\n",
    "        if self.corpus is None:\n",
    "            self.fit()\n",
    "        return self.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63y5g41vpi4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Assigns categories to documents based on Wikipedia page categories.\n",
    "    \"\"\"\n",
    "    def __init__(self, wikipedia_reader):\n",
    "        self.reader = wikipedia_reader\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform file IDs to include category information.\"\"\"\n",
    "        categorized_docs = []\n",
    "        for fileid in X:\n",
    "            # Extract article title from filename\n",
    "            title = fileid.replace('.txt', '').replace('_', ' ')\n",
    "            categories = self.reader.get_categories(title)\n",
    "            categorized_docs.append({\n",
    "                'fileid': fileid,\n",
    "                'title': title,\n",
    "                'categories': categories\n",
    "            })\n",
    "        return categorized_docs\n",
    "\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Preprocesses text from the corpus files with comprehensive text cleaning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    root_path : str\n",
    "        Path to directory containing text files\n",
    "    lowercase : bool, default=True\n",
    "        Convert text to lowercase\n",
    "    remove_stopwords : bool, default=True\n",
    "        Remove common stopwords using NLTK\n",
    "    lemmatize : bool, default=True\n",
    "        Apply lemmatization using WordNetLemmatizer\n",
    "    remove_punctuation : bool, default=True\n",
    "        Remove punctuation and special characters\n",
    "    remove_numbers : bool, default=False\n",
    "        Remove numeric digits\n",
    "    min_token_length : int, default=3\n",
    "        Minimum length for tokens to keep\n",
    "    \"\"\"\n",
    "    def __init__(self, root_path, lowercase=True, remove_stopwords=True, \n",
    "                 lemmatize=True, remove_punctuation=True, \n",
    "                 remove_numbers=False, min_token_length=3):\n",
    "        self.root_path = root_path\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.min_token_length = min_token_length\n",
    "        \n",
    "        # Initialize lemmatizer and stopwords\n",
    "        if self.lemmatize:\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "        if self.remove_stopwords:\n",
    "            from nltk.corpus import stopwords\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Apply comprehensive text cleaning.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Lowercase\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove numbers if requested\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove punctuation if requested\n",
    "        if self.remove_punctuation:\n",
    "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _process_tokens(self, text):\n",
    "        \"\"\"Tokenize and apply token-level processing.\"\"\"\n",
    "        # Simple word tokenization\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Filter short tokens\n",
    "        tokens = [t for t in tokens if len(t) >= self.min_token_length]\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in self.stop_words]\n",
    "        \n",
    "        # Lemmatize\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Read and preprocess text from files.\"\"\"\n",
    "        processed_docs = []\n",
    "        for doc in X:\n",
    "            fileid = doc['fileid'] if isinstance(doc, dict) else doc\n",
    "            filepath = os.path.join(self.root_path, fileid)\n",
    "            \n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    # Apply comprehensive preprocessing\n",
    "                    text = self._clean_text(text)\n",
    "                    text = self._process_tokens(text)\n",
    "                    processed_docs.append({\n",
    "                        'text': text,\n",
    "                        'fileid': fileid,\n",
    "                        'categories': doc.get('categories', {}) if isinstance(doc, dict) else {}\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {fileid}: {e}\")\n",
    "                \n",
    "        return processed_docs\n",
    "\n",
    "\n",
    "class TextTokenizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Tokenizes text into words and sentences.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Tokenize documents into words.\"\"\"\n",
    "        tokenized_docs = []\n",
    "        for doc in X:\n",
    "            text = doc['text'] if isinstance(doc, dict) else doc\n",
    "            # Simple word tokenization\n",
    "            words = nltk.word_tokenize(text)\n",
    "            \n",
    "            tokenized_docs.append({\n",
    "                'tokens': words,\n",
    "                'fileid': doc.get('fileid', '') if isinstance(doc, dict) else '',\n",
    "                'categories': doc.get('categories', {}) if isinstance(doc, dict) else {}\n",
    "            })\n",
    "        return tokenized_docs\n",
    "\n",
    "\n",
    "class BagOfWordVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Converts tokenized documents to bag-of-words representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Build vocabulary from corpus.\"\"\"\n",
    "        self.vocabulary = self.corpus.vocab()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform documents to bag-of-words vectors.\"\"\"\n",
    "        vectors = []\n",
    "        for doc in X:\n",
    "            tokens = doc['tokens'] if isinstance(doc, dict) else doc\n",
    "            # Create frequency distribution for this document\n",
    "            word_freq = nltk.FreqDist(tokens)\n",
    "            vectors.append({\n",
    "                'vector': dict(word_freq),\n",
    "                'fileid': doc.get('fileid', '') if isinstance(doc, dict) else '',\n",
    "                'categories': doc.get('categories', {}) if isinstance(doc, dict) else {}\n",
    "            })\n",
    "        return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "root_path = './articles'\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('corpus', WikipediaCorpusTransformer(root_path=root_path)),\n",
    "    ('categorizer', Categorizer(WikipediaReader())),\n",
    "    ('preprocessor', TextPreprocessor(root_path=root_path)),\n",
    "    ('tokenizer', TextTokenizer()),\n",
    "    ('vectorizer', BagOfWordVectorizer(WikipediaPlaintextCorpus(root_path))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b593dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Run the pipeline to get the vectorized documents\n",
    "transformed_data = pipeline.fit_transform(None)\n",
    "\n",
    "# Create a DataFrame with the results, including preprocessed text\n",
    "X = pd.DataFrame([{\n",
    "    'fileid': doc['fileid'],\n",
    "    'bow': doc['vector'],\n",
    "    'categories': doc['categories']\n",
    "} for doc in transformed_data])\n",
    "\n",
    "# Add preprocessed text by reading from the pipeline's preprocessor output\n",
    "# We need to re-run just the preprocessing steps to get the text\n",
    "preprocessed_data = pipeline.named_steps['preprocessor'].transform(\n",
    "    pipeline.named_steps['categorizer'].transform(\n",
    "        pipeline.named_steps['corpus'].transform(None)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add the preprocessed text to the DataFrame\n",
    "X['preprocessed'] = [doc['text'] for doc in preprocessed_data]\n",
    "\n",
    "# Add tokens from the tokenizer output\n",
    "tokenized_data = pipeline.named_steps['tokenizer'].transform(preprocessed_data)\n",
    "X['tokens'] = [doc['tokens'] for doc in tokenized_data]\n",
    "\n",
    "# Now create the feature matrix\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "x_train = vectorizer.fit_transform(X['bow'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71b25d",
   "metadata": {},
   "source": [
    "## 2. Document Clustering Analysis\n",
    "Apply various clustering algorithms to discover document groups.\n",
    "\n",
    "### 2.1 KMeans Clustering\n",
    "Partition-based clustering with predefined number of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f39e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create and fit KMeans model\n",
    "kmeans = KMeans(n_clusters=8, random_state=0)\n",
    "kmeans.fit(x_train)\n",
    "\n",
    "# Verify that labels were created\n",
    "print(f\"Number of documents clustered: {len(kmeans.labels_)}\")\n",
    "print(f\"Cluster assignments: {kmeans.labels_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ylabel('Clusters')\n",
    "plt.xlabel('Document ID')\n",
    "plt.plot(kmeans.labels_, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0110ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x=kmeans.labels_,  bins=8, density=False)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_articles(c_id, lables):\n",
    "    # Extract title from fileid by removing .txt and replacing underscores\n",
    "    return [X['fileid'][i].replace('.txt', '').replace('_', ' ') for i,l in enumerate(lables) if l == c_id]\n",
    "\n",
    "print(get_cluster_articles(0, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zw0gwy9wcag",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(X, labels, method_name=\"Clustering\"):\n",
    "    \"\"\"\n",
    "    Evaluate clustering quality using multiple metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Feature matrix used for clustering\n",
    "    labels : array-like\n",
    "        Cluster labels assigned to each sample\n",
    "    method_name : str\n",
    "        Name of clustering method for display\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "    import numpy as np\n",
    "    \n",
    "    # Filter out noise points (label -1) for DBSCAN/OPTICS\n",
    "    mask = labels >= 0\n",
    "    n_noise = (labels == -1).sum()\n",
    "    \n",
    "    if mask.sum() < 2:\n",
    "        print(f\"{method_name}: Insufficient non-noise samples for evaluation\")\n",
    "        return {\n",
    "            'method': method_name,\n",
    "            'n_clusters': 0,\n",
    "            'n_noise': n_noise,\n",
    "            'n_samples': len(labels),\n",
    "            'silhouette': None,\n",
    "            'davies_bouldin': None,\n",
    "            'calinski_harabasz': None\n",
    "        }\n",
    "    \n",
    "    # Get unique clusters (excluding noise)\n",
    "    unique_clusters = len(set(labels[mask]))\n",
    "    \n",
    "    if unique_clusters < 2:\n",
    "        print(f\"{method_name}: Only {unique_clusters} cluster(s) found\")\n",
    "        return {\n",
    "            'method': method_name,\n",
    "            'n_clusters': unique_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'n_samples': len(labels),\n",
    "            'silhouette': None,\n",
    "            'davies_bouldin': None,\n",
    "            'calinski_harabasz': None\n",
    "        }\n",
    "    \n",
    "    # Calculate metrics only on non-noise points\n",
    "    X_filtered = X[mask]\n",
    "    labels_filtered = labels[mask]\n",
    "    \n",
    "    metrics = {\n",
    "        'method': method_name,\n",
    "        'n_clusters': unique_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'n_samples': len(labels),\n",
    "        'silhouette': silhouette_score(X_filtered, labels_filtered),\n",
    "        'davies_bouldin': davies_bouldin_score(X_filtered, labels_filtered),\n",
    "        'calinski_harabasz': calinski_harabasz_score(X_filtered, labels_filtered)\n",
    "    }\n",
    "    \n",
    "    # Print formatted results\n",
    "    print(f\"\\n{method_name} Evaluation:\")\n",
    "    print(f\"  Samples: {metrics['n_samples']} ({metrics['n_noise']} noise)\")\n",
    "    print(f\"  Clusters: {metrics['n_clusters']}\")\n",
    "    print(f\"  Silhouette Score: {metrics['silhouette']:.4f} (higher is better, range: -1 to 1)\")\n",
    "    print(f\"  Davies-Bouldin Index: {metrics['davies_bouldin']:.4f} (lower is better)\")\n",
    "    print(f\"  Calinski-Harabasz Score: {metrics['calinski_harabasz']:.2f} (higher is better)\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Example usage (commented out - uncomment to test after running clustering)\n",
    "# metrics_kmeans = evaluate_clustering(x_train, kmeans.labels_, \"KMeans\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zr0shum4u7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elbow(X, k_range=range(2, 21), random_state=42):\n",
    "    \"\"\"\n",
    "    Plot elbow curve to determine optimal number of clusters for KMeans.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Feature matrix for clustering\n",
    "    k_range : range or list\n",
    "        Range of k values to test (default: 2 to 20)\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : Inertia values for each k\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    inertias = []\n",
    "    k_values = list(k_range)\n",
    "    \n",
    "    print(\"Calculating inertia for different k values...\")\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        print(f\"  k={k}: inertia={kmeans.inertia_:.2f}\")\n",
    "    \n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "    plt.ylabel('Inertia (Within-cluster sum of squares)', fontsize=12)\n",
    "    plt.title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(k_values)\n",
    "    \n",
    "    # Try to identify elbow point using second derivative\n",
    "    inertias_arr = np.array(inertias)\n",
    "    if len(inertias_arr) > 2:\n",
    "        # Calculate second derivative (rate of change of slope)\n",
    "        first_deriv = np.diff(inertias_arr)\n",
    "        second_deriv = np.diff(first_deriv)\n",
    "        # Find point where second derivative is maximum (sharpest turn)\n",
    "        elbow_idx = np.argmax(second_deriv) + 2  # +2 because of two diff operations\n",
    "        elbow_k = k_values[elbow_idx]\n",
    "        \n",
    "        # Highlight elbow point\n",
    "        plt.axvline(x=elbow_k, color='r', linestyle='--', alpha=0.7, \n",
    "                    label=f'Suggested k={elbow_k}')\n",
    "        plt.legend()\n",
    "        print(f\"\\nSuggested optimal k (elbow point): {elbow_k}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return inertias\n",
    "\n",
    "\n",
    "# Example usage (commented out - uncomment to run)\n",
    "# inertias = plot_elbow(x_train, k_range=range(2, 21))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ef180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud_for_cluster(c_id, lables):\n",
    "    text = ' '.join([X['preprocessed'][i] for i,l in enumerate(lables) if l == c_id ])\n",
    "\n",
    "    wordcloud = WordCloud(max_font_size=50, max_words=20).generate(text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "wordcloud_for_cluster(0, kmeans.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bd774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "x_train = vectorizer.fit_transform(X['bow'])\n",
    "\n",
    "print(type(x_train))\n",
    "#numpy.ndarray\n",
    "\n",
    "print(x_train)\n",
    "#[[ 15.   0.  10. ...   0.   0.   0.]\n",
    "# [662.   0. 430. ...   0.   0.   0.]\n",
    "# [316.   0. 143. ...   0.   0.   0.]\n",
    "# ...\n",
    "# [319.   0. 217. ...   0.   0.   0.]\n",
    "# [158.   0. 147. ...   0.   0.   0.]\n",
    "# [328.   0. 279. ...   0.   0.   0.]]\n",
    "\n",
    "print(x_train.shape)\n",
    "# (272, 52743)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# array([',', ',1', '.', ..., 'zy', 'zygomaticus', 'zygote'], dtype=object)\n",
    "\n",
    "print(len(vectorizer.get_feature_names_out()))\n",
    "# 52743\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_reduce(vec_list, dimensions=2):\n",
    "    return PCA(dimensions).fit_transform(vec_list)\n",
    "\n",
    "def d2_plot(data):\n",
    "    plt.plot(data, 'o')\n",
    "\n",
    "d2_plot(pca_reduce(x_train,2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2_plot(data):\n",
    "    plt.plot(data, '.')\n",
    "\n",
    "d2_plot(pca_reduce(x_train,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a5cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d3_plot(data):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    for _, v in enumerate(data[:90]):\n",
    "        ax.scatter(v[0],v[1], v[2],marker='.', color='r')\n",
    "    for _, v in enumerate(data[90:180]):\n",
    "        ax.scatter(v[0],v[1], v[2],marker='.', color='g')\n",
    "    for _, v in enumerate(data[180:]):\n",
    "        ax.scatter(v[0],v[1], v[2],marker ='.', color='b')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "d3_plot(pca_reduce(x_train,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "x_train = X['preprocessed'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "# (272, 40337)\n",
    "\n",
    "print(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e98987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "vocab = corpus.vocab()\n",
    "vector_lookup = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "def word_vector(tokens):\n",
    "    return np.array([\n",
    "        vector_lookup[token]\n",
    "        for token in tokens\n",
    "        if token in vocab and token in vector_lookup\n",
    "    ])\n",
    "\n",
    "X['word_vector'] = X['tokens'].apply(lambda tokens: word_vector(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_length = np.array([len(tokens) for tokens in X['word_vector'].to_numpy().flatten()])\n",
    "\n",
    "print(word_vector_length[:5])\n",
    "# [760, 157, 7566, 2543, 2086]\n",
    "\n",
    "bins=int(np.max(word_vector_length)/1000)\n",
    "\n",
    "plt.hist(x=word_vector_length,  bins=bins, density=False)\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean: {word_vector_length.mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51aca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_word_vectors(vec_list, padding_value):\n",
    "    res = []\n",
    "    for vec in vec_list:\n",
    "        con = np.array([v for v in vec]).reshape(-1)\n",
    "        con_padded = np.pad(con, (0, padding_value))\n",
    "        con_truncated = con_padded[:padding_value]\n",
    "        res.append(con_truncated)\n",
    "    return np.array(res)\n",
    "\n",
    "def pca_reduce(vec_list, n_components):\n",
    "    return PCA(n_components).fit_transform(vec_list)\n",
    "\n",
    "# Use the X DataFrame we already created earlier (no pickle needed!)\n",
    "x_train = X['word_vector'].to_numpy()\n",
    "\n",
    "x_train_padded = pad_word_vectors(x_train, 300000)\n",
    "\n",
    "x_train_2d = pca_reduce(x_train_padded, 2)\n",
    "x_train_3d = pca_reduce(x_train_padded, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b10b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit KMeans on the 2D PCA-reduced vectors to avoid \"setting an array element with a sequence\" error\n",
    "model = KMeans(n_clusters=8, random_state=0).fit(x_train_2d)\n",
    "print(model)\n",
    "print(model.get_params())\n",
    "print(model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e23173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=8, random_state=0, n_init=\"auto\").fit(x_train_2d)\n",
    "\n",
    "print(model.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d74b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "\n",
    "x_train = X['preprocessed'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310f8f3",
   "metadata": {},
   "source": [
    "### 2.2 DBSCAN Clustering\n",
    "Density-based clustering that can find arbitrary-shaped clusters and identify noise points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Clustering with Parameter Tuning\n",
    "# DBSCAN works poorly in high-dimensional spaces, so we reduce dimensionality first\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Reduce dimensionality to make DBSCAN effective\n",
    "print(\"Reducing dimensionality for DBSCAN...\")\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "x_train_pca50 = pca.fit_transform(x_train.toarray() if hasattr(x_train, 'toarray') else x_train)\n",
    "print(f\"Reduced from {x_train.shape[1]} to 50 dimensions\")\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2%}\\n\")\n",
    "\n",
    "# Step 2: Find optimal eps using k-distance graph\n",
    "print(\"Finding optimal eps parameter using k-distance graph...\")\n",
    "min_samples = 5  # Typical: 2*dim, but we'll use 5 for start\n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors_fit = neighbors.fit(x_train_pca50)\n",
    "distances, indices = neighbors_fit.kneighbors(x_train_pca50)\n",
    "\n",
    "# Sort distances to k-th nearest neighbor\n",
    "k_distances = np.sort(distances[:, min_samples-1], axis=0)\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_distances)\n",
    "plt.ylabel(f'{min_samples}-Nearest Neighbor Distance', fontsize=12)\n",
    "plt.xlabel('Points sorted by distance', fontsize=12)\n",
    "plt.title('K-Distance Graph for DBSCAN eps Selection', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=np.percentile(k_distances, 90), color='r', linestyle='--', \n",
    "            alpha=0.7, label=f'90th percentile: {np.percentile(k_distances, 90):.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Use 90th percentile as eps (common heuristic for elbow point)\n",
    "optimal_eps = np.percentile(k_distances, 90)\n",
    "print(f\"\\nSuggested eps (90th percentile): {optimal_eps:.2f}\")\n",
    "\n",
    "# Step 3: Run DBSCAN with tuned parameters\n",
    "print(f\"\\nRunning DBSCAN with eps={optimal_eps:.2f}, min_samples={min_samples}...\")\n",
    "dbscan_model = DBSCAN(eps=optimal_eps, min_samples=min_samples)\n",
    "dbscan_labels = dbscan_model.fit_predict(x_train_pca50)\n",
    "\n",
    "# Step 4: Evaluate results\n",
    "print(f\"\\nDBSCAN Results:\")\n",
    "print(f\"  Parameters: eps={optimal_eps:.2f}, min_samples={min_samples}\")\n",
    "unique_labels = set(dbscan_labels)\n",
    "n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "print(f\"  Clusters found: {n_clusters}\")\n",
    "print(f\"  Noise points: {n_noise} ({n_noise/len(dbscan_labels)*100:.1f}%)\")\n",
    "print(f\"  Cluster sizes: {[(label, list(dbscan_labels).count(label)) for label in unique_labels if label != -1]}\")\n",
    "\n",
    "# Step 5: Evaluate clustering quality\n",
    "if n_clusters >= 2:\n",
    "    dbscan_metrics = evaluate_clustering(x_train_pca50, dbscan_labels, \"DBSCAN\")\n",
    "else:\n",
    "    print(\"\\nInsufficient clusters for quality metrics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3969cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def pca_reduce(vec_list, n_components):\n",
    "    return TruncatedSVD(n_components).fit_transform(vec_list)\n",
    "\n",
    "x_train_3d = pca_reduce(x_train, 3)\n",
    "\n",
    "model = DBSCAN().fit(x_train_3d)\n",
    "\n",
    "print(model.get_params())\n",
    "print(model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90669f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "x_train = X['preprocessed'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train).todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd31b5e",
   "metadata": {},
   "source": [
    "### 2.3 OPTICS Clustering\n",
    "Ordering Points To Identify Clustering Structure - density-based clustering with variable density.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f75719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTICS Clustering with Parameter Tuning\n",
    "# Like DBSCAN, OPTICS works better with dimensionality reduction\n",
    "\n",
    "from sklearn.cluster import OPTICS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Use the PCA-reduced data from DBSCAN (if not already created, create it)\n",
    "if 'x_train_pca50' not in globals():\n",
    "    from sklearn.decomposition import PCA\n",
    "    print(\"Reducing dimensionality for OPTICS...\")\n",
    "    pca = PCA(n_components=50, random_state=42)\n",
    "    x_train_pca50 = pca.fit_transform(x_train.toarray() if hasattr(x_train, 'toarray') else x_train)\n",
    "    print(f\"Reduced to 50 dimensions\\n\")\n",
    "\n",
    "# Try different min_samples values\n",
    "print(\"Testing OPTICS with different min_samples values...\")\n",
    "min_samples_options = [5, 10, 15]\n",
    "best_model = None\n",
    "best_metrics = None\n",
    "best_min_samples = None\n",
    "\n",
    "for min_samples in min_samples_options:\n",
    "    print(f\"\\n--- Testing min_samples={min_samples} ---\")\n",
    "    \n",
    "    # Run OPTICS with finite max_eps to help with convergence\n",
    "    optics_model = OPTICS(\n",
    "        min_samples=min_samples,\n",
    "        max_eps=10.0,  # Finite max_eps helps prevent single-cluster output\n",
    "        cluster_method='dbscan',  # Use DBSCAN extraction method\n",
    "        metric='euclidean'\n",
    "    )\n",
    "    optics_labels = optics_model.fit_predict(x_train_pca50)\n",
    "    \n",
    "    # Check results\n",
    "    unique_labels = set(optics_labels)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "    n_noise = list(optics_labels).count(-1)\n",
    "    \n",
    "    print(f\"  Clusters found: {n_clusters}\")\n",
    "    print(f\"  Noise points: {n_noise} ({n_noise/len(optics_labels)*100:.1f}%)\")\n",
    "    \n",
    "    # Evaluate if we have at least 2 clusters\n",
    "    if n_clusters >= 2 and n_clusters < len(optics_labels) // 2:\n",
    "        try:\n",
    "            metrics = evaluate_clustering(x_train_pca50, optics_labels, f\"OPTICS (min_samples={min_samples})\")\n",
    "            if best_metrics is None or (metrics['silhouette'] and metrics['silhouette'] > (best_metrics.get('silhouette') or -1)):\n",
    "                best_model = optics_model\n",
    "                best_metrics = metrics\n",
    "                best_min_samples = min_samples\n",
    "        except:\n",
    "            print(f\"  Could not evaluate clustering for min_samples={min_samples}\")\n",
    "\n",
    "# Use best model\n",
    "if best_model is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Best OPTICS configuration: min_samples={best_min_samples}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    optics_best_labels = best_model.labels_\n",
    "    \n",
    "    # Plot reachability distance to visualize cluster structure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Reachability plot\n",
    "    space = np.arange(len(x_train_pca50))\n",
    "    reachability = best_model.reachability_[best_model.ordering_]\n",
    "    labels = best_model.labels_[best_model.ordering_]\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors = ['g.', 'r.', 'b.', 'y.', 'c.', 'm.', 'k.', 'orange']\n",
    "    for klass, color in zip(range(0, max(labels) + 1), colors):\n",
    "        Xk = space[labels == klass]\n",
    "        Rk = reachability[labels == klass]\n",
    "        plt.plot(Xk, Rk, color, alpha=0.5)\n",
    "    plt.plot(space[labels == -1], reachability[labels == -1], 'k+', alpha=0.1, label='Noise')\n",
    "    plt.ylabel('Reachability Distance', fontsize=12)\n",
    "    plt.xlabel('Sample Order', fontsize=12)\n",
    "    plt.title('Reachability Plot', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cluster distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    cluster_counts = [list(optics_best_labels).count(i) for i in range(max(optics_best_labels) + 1) if i != -1]\n",
    "    cluster_ids = [i for i in range(max(optics_best_labels) + 1) if i != -1]\n",
    "    plt.bar(cluster_ids, cluster_counts)\n",
    "    plt.xlabel('Cluster ID', fontsize=12)\n",
    "    plt.ylabel('Number of Documents', fontsize=12)\n",
    "    plt.title('Cluster Size Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nWarning: Could not find suitable OPTICS clustering parameters\")\n",
    "    print(\"All tested configurations resulted in too few clusters or poor quality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded = pad_word_vectors(x_train,300000)\n",
    "\n",
    "n_clusters = 5\n",
    "model = KMeans(n_clusters, random_state=0, n_init=\"auto\").fit(x_train_padded )\n",
    "\n",
    "print(model.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded = pad_word_vectors(x_train,300000)\n",
    "x_train_3d = pca_reduce(x_train_padded,3)\n",
    "\n",
    "n_clusters = 5\n",
    "model = KMeans(n_clusters, random_state=0, n_init=\"auto\").fit(x_train_3d)\n",
    "\n",
    "print(model.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded = pad_word_vectors(x_train,300000)\n",
    "\n",
    "model = DBSCAN().fit(x_train_3d)\n",
    "\n",
    "print(model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568713d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded = pad_word_vectors(x_train,300000)\n",
    "\n",
    "model = OPTICS(min_samples=10).fit(np.array(x_train_padded))\n",
    "\n",
    "print(model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_3d = pca_reduce(x_train_padded,3)\n",
    "\n",
    "model = OPTICS(min_samples=10).fit(np.array(x_train_3d))\n",
    "\n",
    "print(model.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OPTICS(min_samples=5, metric='minkowski').fit(np.array(x_train_3d))\n",
    "\n",
    "print(model.get_params())\n",
    "print(model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d95b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi as wiki_api\n",
    "\n",
    "wiki = wiki_api.Wikipedia(\n",
    "                language = 'en',\n",
    "                extract_format=wiki_api.ExtractFormat.WIKI,\n",
    "                user_agent = 'jmoses126@gmail.com')\n",
    "\n",
    "p = wiki.page('Vehicular_automation')\n",
    "\n",
    "print(len(p.categories))\n",
    "\n",
    "print([name.replace('Category:', '') for name in p.categories.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c78976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_categories(article_name):\n",
    "    wiki = wiki_api.Wikipedia(language = 'en', user_agent = 'jmoses126@gmail.com')\n",
    "\n",
    "    p = wiki.page(article_name)\n",
    "\n",
    "    if not p.exists():\n",
    "        return None\n",
    "\n",
    "    # Add proper headers to the request\n",
    "    headers = {\n",
    "        'User-Agent': 'jmoses126@gmail.com'\n",
    "    }\n",
    "    \n",
    "    r = requests.get(p.fullurl, headers=headers)\n",
    "    html = r.text\n",
    "    \n",
    "    # Updated regex to match the current Wikipedia HTML structure\n",
    "    catlinks_regexp = re.compile(r'id=\"mw-normal-catlinks\".*?</div>', re.DOTALL)\n",
    "    catnames_regexp = re.compile(r'title=\"Category:(.*?)\"')\n",
    "\n",
    "    cat_matches = catlinks_regexp.findall(html)\n",
    "    \n",
    "    if len(cat_matches) == 0:\n",
    "        return ['Uncategorized']\n",
    "\n",
    "    cat_src = cat_matches[0]\n",
    "    cats = catnames_regexp.findall(cat_src)\n",
    "\n",
    "    if len(cats) == 0:\n",
    "        return ['Uncategorized']\n",
    "    else:\n",
    "        return cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_categories('Artificial_intelligence')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb08f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_categories('Vehicular_automation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('corpus', WikipediaCorpusTransformer(root_path=root_path)),\n",
    "    ('categorizer', Categorizer(WikipediaReader())),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.wiki = wiki_api.Wikipedia(language='en', user_agent='jmoses126@gmail.com')\n",
    "        self.catlinks_regexp = re.compile(r'class=\"mw-normal-catlinks\".*?<\\/div>')\n",
    "        self.catnames_regexp = re.compile(r'<a.*?>(.*?)<\\/a>')\n",
    "\n",
    "    def get_categories(self, article_name):\n",
    "        p = self.wiki.page(article_name)\n",
    "\n",
    "         # Add proper headers to the request\n",
    "        headers = {\n",
    "            'user-agent': 'jmoses126@gmail.com'\n",
    "        }\n",
    "        if p.exists:\n",
    "            try:\n",
    "                r = requests.get(p.fullurl, headers=headers)\n",
    "                html = r.text.replace('\\r', '').replace('\\n', '')\n",
    "\n",
    "                cat_src = self.catlinks_regexp.findall(html)[0]\n",
    "                cats = self.catnames_regexp.findall(cat_src)\n",
    "\n",
    "                if len(cats) > 0:\n",
    "                    return dict.fromkeys(cats[1:len(cats)], 1)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        return {'Uncategorized':1}\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        X['categories'] = X['title'].apply(lambda title: self.get_categories(title))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('corpus', WikipediaCorpusTransformer(root_path=root_path)),\n",
    "    ('categorizer', Categorizer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619965e",
   "metadata": {},
   "source": [
    "## 4. Multi-Label Classification\n",
    "Train a model to predict Wikipedia article categories from text content.\n",
    "\n",
    "### 4.1 Data Preparation\n",
    "Split data into train/test sets and convert to Spacy format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(X, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from copy import copy\n",
    "from itertools import chain\n",
    "\n",
    "def convert_multi_label(df, filename):\n",
    "    db = DocBin()\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    total = len(df.index)\n",
    "    print(f'{time()}: start processing {filename} with {total} files')\n",
    "\n",
    "    categories_list = set(list(chain.from_iterable([list(d) for d in df[\"categories\"].tolist()])))\n",
    "    categories_dict = { cat: 0 for cat in categories_list }\n",
    "    #print(categories_dict)\n",
    "\n",
    "    count = 0\n",
    "    for _, row in df.iterrows():\n",
    "        count += 1\n",
    "        print(f'Processing {count}/{total}')\n",
    "        doc = nlp(row['preprocessed'])  # Changed from 'raw' to 'preprocessed'\n",
    "        cats = copy(categories_dict)\n",
    "        for cat in row['categories']:\n",
    "            cats[cat] = 1\n",
    "\n",
    "        doc.cats = cats\n",
    "        #print(doc.cats)\n",
    "        db.add(doc)\n",
    "\n",
    "    print(f'{time()}: finish processing {filename}')\n",
    "    db.to_disk(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_multi_label(train, 'wikipedia_multi_label_train2.spacy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_multi_label(test, 'wikipedia_multi_label_train2.spacy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cpgzmdfsctm",
   "metadata": {},
   "source": [
    "### 4.2 Model Training\n",
    "Train a multi-label text classification model using Spacy's textcat component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4z4k2fsez",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spacy training configuration for multi-label text classification\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.tokens import DocBin\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory for the model\n",
    "output_dir = Path(\"./wikipedia_textcat_model\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Creating blank Spacy model with textcat component...\")\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add the textcat component to the pipeline\n",
    "textcat = nlp.add_pipe(\"textcat_multilabel\", last=True)\n",
    "\n",
    "# Get all unique categories from the training data\n",
    "print(\"\\nLoading training data to get categories...\")\n",
    "train_docbin = DocBin().from_disk(\"wikipedia_multi_label_train2.spacy\")\n",
    "train_docs = list(train_docbin.get_docs(nlp.vocab))\n",
    "\n",
    "# Get all categories from the first document (they all have the same keys)\n",
    "if train_docs:\n",
    "    categories = list(train_docs[0].cats.keys())\n",
    "    print(f\"Found {len(categories)} categories:\")\n",
    "    print(f\"Categories: {sorted(categories)[:10]}...\" if len(categories) > 10 else sorted(categories))\n",
    "    \n",
    "    # Add labels to textcat\n",
    "    for category in categories:\n",
    "        textcat.add_label(category)\n",
    "    \n",
    "    print(f\"\\nAdded {len(categories)} labels to textcat component\")\n",
    "else:\n",
    "    print(\"Warning: No training documents found!\")\n",
    "\n",
    "print(\"\\nModel configuration complete!\")\n",
    "print(f\"Pipeline components: {nlp.pipe_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z5tcg6vxxe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training on {len(train_docs)} documents\\n\")\n",
    "\n",
    "# Initialize the model\n",
    "nlp.initialize(lambda: [Example.from_dict(doc, {\"cats\": doc.cats}) for doc in train_docs])\n",
    "\n",
    "# Training parameters\n",
    "n_iter = 20  # Number of training iterations\n",
    "batch_size = 8\n",
    "dropout = 0.2\n",
    "\n",
    "# Get the optimizer\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "# Training loop\n",
    "losses_history = []\n",
    "for epoch in range(n_iter):\n",
    "    random.shuffle(train_docs)\n",
    "    losses = {}\n",
    "    \n",
    "    # Create batches\n",
    "    batches = [train_docs[i:i+batch_size] for i in range(0, len(train_docs), batch_size)]\n",
    "    \n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for doc in batch:\n",
    "            # Create Example from doc\n",
    "            example = Example.from_dict(doc, {\"cats\": doc.cats})\n",
    "            examples.append(example)\n",
    "        \n",
    "        # Update the model\n",
    "        nlp.update(examples, drop=dropout, losses=losses, sgd=optimizer)\n",
    "    \n",
    "    losses_history.append(losses.get('textcat_multilabel', 0))\n",
    "    \n",
    "    # Print progress every 5 iterations\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Iteration {epoch + 1}/{n_iter} - Loss: {losses.get('textcat_multilabel', 0):.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete! Final loss: {losses_history[-1]:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "print(f\"\\nSaving model to {output_dir}...\")\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Plot training loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_iter + 1), losses_history, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ow1mql751rn",
   "metadata": {},
   "source": [
    "### 4.3 Model Evaluation\n",
    "Evaluate the trained model on the test set and analyze performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o7n3f41i5ei",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "print(\"Evaluating model on test set...\")\n",
    "\n",
    "# Load the trained model\n",
    "nlp_trained = spacy.load(output_dir)\n",
    "\n",
    "# Load test data\n",
    "test_docbin = DocBin().from_disk(\"wikipedia_multi_label_train2.spacy\")  # Note: should be test file\n",
    "test_docs_raw = list(test_docbin.get_docs(nlp_trained.vocab))\n",
    "\n",
    "print(f\"Loaded {len(test_docs_raw)} test documents\")\n",
    "\n",
    "# Evaluate on test set\n",
    "from sklearn.metrics import classification_report, hamming_loss, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for evaluation\n",
    "y_true = []\n",
    "y_pred = []\n",
    "category_names = sorted(test_docs_raw[0].cats.keys())\n",
    "\n",
    "print(f\"\\nEvaluating on {len(category_names)} categories...\")\n",
    "\n",
    "for doc in test_docs_raw:\n",
    "    # Get true labels\n",
    "    true_labels = [doc.cats[cat] for cat in category_names]\n",
    "    y_true.append(true_labels)\n",
    "    \n",
    "    # Get predictions\n",
    "    predicted = nlp_trained(doc.text)\n",
    "    pred_labels = [predicted.cats[cat] for cat in category_names]\n",
    "    y_pred.append(pred_labels)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred_probs = np.array(y_pred)\n",
    "\n",
    "# Convert probabilities to binary predictions (threshold = 0.5)\n",
    "y_pred_binary = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MULTI-LABEL CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Overall metrics\n",
    "hamming = hamming_loss(y_true, y_pred_binary)\n",
    "accuracy_subset = accuracy_score(y_true, y_pred_binary)\n",
    "\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"  Hamming Loss: {hamming:.4f} (lower is better)\")\n",
    "print(f\"  Subset Accuracy: {accuracy_subset:.4f} (exact match of all labels)\")\n",
    "\n",
    "# Per-label metrics\n",
    "print(f\"\\nPer-Category Performance:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Calculate precision, recall, F1 for each category\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precisions, recalls, f1s, supports = precision_recall_fscore_support(\n",
    "    y_true, y_pred_binary, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Category': category_names,\n",
    "    'Precision': precisions,\n",
    "    'Recall': recalls,\n",
    "    'F1-Score': f1s,\n",
    "    'Support': supports\n",
    "})\n",
    "\n",
    "# Sort by F1 score\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "# Show top 10 and bottom 10\n",
    "print(\"\\nTop 10 Categories (by F1-Score):\")\n",
    "print(results_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom 10 Categories (by F1-Score):\")\n",
    "print(results_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Macro and micro averages\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred_binary, average='macro', zero_division=0\n",
    ")\n",
    "precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred_binary, average='micro', zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Average Metrics:\")\n",
    "print(f\"  Macro-averaged F1: {f1_macro:.4f} (unweighted mean)\")\n",
    "print(f\"  Micro-averaged F1: {f1_micro:.4f} (weighted by support)\")\n",
    "print(f\"  Macro-averaged Precision: {precision_macro:.4f}\")\n",
    "print(f\"  Macro-averaged Recall: {recall_macro:.4f}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7v0eb45jbp3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. F1 scores for top categories\n",
    "ax1 = axes[0, 0]\n",
    "top_20 = results_df.head(20)\n",
    "ax1.barh(range(len(top_20)), top_20['F1-Score'].values)\n",
    "ax1.set_yticks(range(len(top_20)))\n",
    "ax1.set_yticklabels(top_20['Category'].values, fontsize=8)\n",
    "ax1.set_xlabel('F1-Score', fontsize=10)\n",
    "ax1.set_title('Top 20 Categories by F1-Score', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# 2. Precision vs Recall scatter\n",
    "ax2 = axes[0, 1]\n",
    "scatter = ax2.scatter(results_df['Recall'], results_df['Precision'], \n",
    "                     s=results_df['Support']*10, alpha=0.6, c=results_df['F1-Score'],\n",
    "                     cmap='viridis')\n",
    "ax2.set_xlabel('Recall', fontsize=10)\n",
    "ax2.set_ylabel('Precision', fontsize=10)\n",
    "ax2.set_title('Precision vs Recall (bubble size = support)', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.plot([0, 1], [0, 1], 'r--', alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax2, label='F1-Score')\n",
    "\n",
    "# 3. Distribution of F1 scores\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(results_df['F1-Score'].values, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(f1_macro, color='r', linestyle='--', linewidth=2, label=f'Macro Avg: {f1_macro:.3f}')\n",
    "ax3.axvline(f1_micro, color='g', linestyle='--', linewidth=2, label=f'Micro Avg: {f1_micro:.3f}')\n",
    "ax3.set_xlabel('F1-Score', fontsize=10)\n",
    "ax3.set_ylabel('Number of Categories', fontsize=10)\n",
    "ax3.set_title('Distribution of F1-Scores Across Categories', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Category support distribution\n",
    "ax4 = axes[1, 1]\n",
    "support_sorted = results_df.sort_values('Support', ascending=False)\n",
    "ax4.bar(range(min(30, len(support_sorted))), support_sorted['Support'].head(30).values)\n",
    "ax4.set_xlabel('Category (sorted by support)', fontsize=10)\n",
    "ax4.set_ylabel('Number of Test Samples', fontsize=10)\n",
    "ax4.set_title('Top 30 Categories by Test Set Support', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Visualization complete!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3higg8ap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get a few test examples\n",
    "num_examples = 5\n",
    "for i in range(min(num_examples, len(test_docs_raw))):\n",
    "    doc_raw = test_docs_raw[i]\n",
    "    \n",
    "    # Get prediction\n",
    "    predicted = nlp_trained(doc_raw.text)\n",
    "    \n",
    "    # Get true and predicted categories (above threshold)\n",
    "    threshold = 0.5\n",
    "    true_cats = [cat for cat in category_names if doc_raw.cats[cat] == 1]\n",
    "    pred_cats = [(cat, score) for cat, score in predicted.cats.items() if score > threshold]\n",
    "    pred_cats_sorted = sorted(pred_cats, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Text preview: {doc_raw.text[:150]}...\")\n",
    "    print(f\"\\nTrue categories ({len(true_cats)}): {', '.join(true_cats[:5])}\")\n",
    "    if len(true_cats) > 5:\n",
    "        print(f\"  ... and {len(true_cats) - 5} more\")\n",
    "    \n",
    "    print(f\"\\nPredicted categories ({len(pred_cats_sorted)}):\")\n",
    "    for cat, score in pred_cats_sorted[:5]:\n",
    "        match = \"\" if cat in true_cats else \"\"\n",
    "        print(f\"  {match} {cat}: {score:.3f}\")\n",
    "    if len(pred_cats_sorted) > 5:\n",
    "        print(f\"  ... and {len(pred_cats_sorted) - 5} more\")\n",
    "    \n",
    "    # Calculate accuracy for this example\n",
    "    correct = len(set(true_cats) & set([c for c, _ in pred_cats_sorted]))\n",
    "    precision_ex = correct / len(pred_cats_sorted) if pred_cats_sorted else 0\n",
    "    recall_ex = correct / len(true_cats) if true_cats else 0\n",
    "    \n",
    "    print(f\"\\nExample metrics: Precision={precision_ex:.2f}, Recall={recall_ex:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc04350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cats(model, article):\n",
    "    name = article[\"title\"]\n",
    "    text = article[\"raw\"]\n",
    "    expected_cats = article[\"categories\"]\n",
    "\n",
    "    nlp = spacy.load(f'{model}/model-best')\n",
    "    doc = nlp(text)\n",
    "    estimated_cats = (sorted(doc.cats.items(), key=lambda i:float(i[1]), reverse=True))\n",
    "\n",
    "    print(f'Article {name} || model {model}\"')\n",
    "    print(expected_cats)\n",
    "    print(estimated_cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f2718",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d04ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
